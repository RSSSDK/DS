{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Capstone Project - Car accident severity  (Week 2)\n### Applied Data Science Capstone by IBM/Coursera"}, {"metadata": {}, "cell_type": "markdown", "source": "## Table of contents\n* [Introduction: Business Problem](#introduction)\n* [Data](#data)\n* [Methodology](#methodology)\n* [Analysis](#analysis)\n* [Results and Discussion](#results)\n* [Conclusion](#conclusion)"}, {"metadata": {}, "cell_type": "markdown", "source": "\n\n## Introduction: Business Problem <a name=\"introduction\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Because of the increase in the number of cars and the raise of accidents happened in the roads too, this cause injuries or fatalities to people, waste time and cost for police and rescue vehicles, traffic delay in the area.\n\nThe Seattle Department of Transportation (SODT) objective to improve new model aims to predict the severity of car accidents and main variables that strongly related to it, in order to help decisions based to make campaigns or improvements to decrease severe accidents based on certain conditions. And detect intersection have car accidents with hight servity ."}, {"metadata": {}, "cell_type": "markdown", "source": "## Data <a name=\"data\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "**Data Understanding**\n\nIn this phase, and based on definition of our problem, many factors that will influence our decission . I will use csv file provided by Coursera. The main attributes I will use as independent variables are: WEATHER, ROADCOND and LIGHTCOND, that adequate to train my machine learning model. Also, I will assess the condition of these attributes by looking for skewed information and the correlations between them and the severity of the accident, so the \"SEVERITYCODE\" used as labeld data  property damage (class 1) or injury (class 2) .\n\n**Data preparation**\n\nIn this phase I will build the final dataset that fed my model by:\n\n    *Filling missing data\n    * Decodes categoral data\n    *Finally, balance and normalize all data."}, {"metadata": {}, "cell_type": "markdown", "source": "**Methodolegy**\n\nAfter the data prepeared and processed i will splitt it by 0.3.\n\nWe will use the following models:\n\n K-Nearest Neighbor (KNN)\n     KNN will help us predict the severity code of an outcome by finding the most similar to data point within k distance.\n\n Decision Tree\n     A decision tree model gives us a layout of all possible outcomes so we can fully analyze the concequences of a decision. It context, the decision tree observes all possible outcomes of different weather conditions.\n\n Logistic Regression\n        Because our dataset only provides us with two severity code outcomes, our model will only predict one of those two classes. This makes our data binary, which is perfect to use with logistic regression.\nThen make evaluation on test data ."}, {"metadata": {}, "cell_type": "markdown", "source": "**Analysis**"}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd \nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_04291aa3be0641f2890aaf65918fae54 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='HD_xp4nQQvJuSvzGoPsGcT4zx5DQvVIPmPSxVMdoU2LO',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_04291aa3be0641f2890aaf65918fae54.get_object(Bucket='pythonbasics-donotdelete-pr-s1eqxttrtxkkh3',Key='Data-Collisions.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_1 = pd.read_csv(body)\ndf_data_1.head()\n#df = pd.read_csv(\"Data-Collisions.csv\")\ndf= df_data_1", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dataset = df.drop(columns = [\"SEVERITYCODE.1\",\"X\",\"Y\",\"OBJECTID\",\"INCKEY\",\"COLDETKEY\",\"REPORTNO\",\"STATUS\",\"ADDRTYPE\",\"INTKEY\",\"LOCATION\",\"EXCEPTRSNCODE\",\"EXCEPTRSNDESC\",\"SEVERITYDESC\",\"COLLISIONTYPE\",\"PERSONCOUNT\",\"PEDCOUNT\",\"PEDCYLCOUNT\",\"VEHCOUNT\",\"INCDATE\",\"INCDTTM\",\"JUNCTIONTYPE\",\"SDOT_COLCODE\",\"SDOT_COLDESC\",\"INATTENTIONIND\",\"UNDERINFL\",\"PEDROWNOTGRNT\",\"SDOTCOLNUM\",\"SPEEDING\",\"ST_COLCODE\",\"ST_COLDESC\",\"SEGLANEKEY\",\"CROSSWALKKEY\",\"HITPARKEDCAR\"] )\ndataset.head ()", "execution_count": 5, "outputs": [{"output_type": "execute_result", "execution_count": 5, "data": {"text/plain": "   SEVERITYCODE   WEATHER ROADCOND                LIGHTCOND\n0             2  Overcast      Wet                 Daylight\n1             1   Raining      Wet  Dark - Street Lights On\n2             1  Overcast      Dry                 Daylight\n3             1     Clear      Dry                 Daylight\n4             2   Raining      Wet                 Daylight", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SEVERITYCODE</th>\n      <th>WEATHER</th>\n      <th>ROADCOND</th>\n      <th>LIGHTCOND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Overcast</td>\n      <td>Wet</td>\n      <td>Daylight</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Raining</td>\n      <td>Wet</td>\n      <td>Dark - Street Lights On</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Overcast</td>\n      <td>Dry</td>\n      <td>Daylight</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Clear</td>\n      <td>Dry</td>\n      <td>Daylight</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Raining</td>\n      <td>Wet</td>\n      <td>Daylight</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Find Missing Data and Uniques Values "}, {"metadata": {}, "cell_type": "code", "source": "dataset.nunique()", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "SEVERITYCODE     2\nWEATHER         11\nROADCOND         9\nLIGHTCOND        9\ndtype: int64"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "dataset.isna().sum()", "execution_count": 7, "outputs": [{"output_type": "execute_result", "execution_count": 7, "data": {"text/plain": "SEVERITYCODE       0\nWEATHER         5081\nROADCOND        5012\nLIGHTCOND       5170\ndtype: int64"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Change categories  to Numbers "}, {"metadata": {}, "cell_type": "code", "source": "dataset[\"WEATHER\"] = dataset[\"WEATHER\"].astype('category')\ndataset[\"ROADCOND\"] = dataset[\"ROADCOND\"].astype('category')\ndataset[\"LIGHTCOND\"] = dataset[\"LIGHTCOND\"].astype('category')\n\ndataset[\"WEATHER_CAT\"] = dataset[\"WEATHER\"].cat.codes\ndataset[\"ROADCOND_CAT\"] = dataset[\"ROADCOND\"].cat.codes\ndataset[\"LIGHTCOND_CAT\"] = dataset[\"LIGHTCOND\"].cat.codes\n", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Data Balance"}, {"metadata": {}, "cell_type": "code", "source": "dataset.groupby([\"SEVERITYCODE\"]).count()", "execution_count": 9, "outputs": [{"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "              WEATHER  ROADCOND  LIGHTCOND  WEATHER_CAT  ROADCOND_CAT  \\\nSEVERITYCODE                                                            \n1              132488    132533     132405       136485        136485   \n2               57104     57128      57098        58188         58188   \n\n              LIGHTCOND_CAT  \nSEVERITYCODE                 \n1                    136485  \n2                     58188  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>WEATHER</th>\n      <th>ROADCOND</th>\n      <th>LIGHTCOND</th>\n      <th>WEATHER_CAT</th>\n      <th>ROADCOND_CAT</th>\n      <th>LIGHTCOND_CAT</th>\n    </tr>\n    <tr>\n      <th>SEVERITYCODE</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>132488</td>\n      <td>132533</td>\n      <td>132405</td>\n      <td>136485</td>\n      <td>136485</td>\n      <td>136485</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>57104</td>\n      <td>57128</td>\n      <td>57098</td>\n      <td>58188</td>\n      <td>58188</td>\n      <td>58188</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.utils import resample \n\ndataset_1 = dataset[dataset.SEVERITYCODE==1]\ndataset_2 = dataset[dataset.SEVERITYCODE==2]\n\ndatset_1_sample = resample(dataset_1,replace = False ,n_samples=58188,random_state=99)\n\ndatset_balanced =  pd.concat( [datset_1_sample,dataset_2] )\n", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Split data to Training and Testing"}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np \n\nX = np.asarray(datset_balanced[[\"WEATHER_CAT\",\"ROADCOND_CAT\",\"LIGHTCOND_CAT\"]])\ny = np.asarray(datset_balanced[\"SEVERITYCODE\"])\n\nfrom sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test ,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=3)\n", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int8 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int8 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n", "name": "stderr"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Analysis by Model <a name=\"analysis\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Let's perform some models (Logistc,)"}, {"metadata": {}, "cell_type": "markdown", "source": "**1) Using Logistic Regression Model**"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\nLRBest = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**2) KNN**"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.neighbors import KNeighborsClassifier\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nConfustionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)  \n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\nKNNBest = KNeighborsClassifier(n_neighbors = mean_acc.argmax()+1).fit(X_train,y_train)", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**3)  DecisionTree**"}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.tree import DecisionTreeClassifier\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\nConfustionMx = [];\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    \n    DTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = n)\n    DTree.fit(X_train,y_train)\n    yhat=DTree.predict(X_test)\n    \n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n\nDTreeBest = DecisionTreeClassifier(criterion=\"entropy\", max_depth = mean_acc.argmax()+1).fit(X_train,y_train)", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn import svm\nSVMBest = svm.SVC(kernel='linear' ,gamma='auto').fit(X_train, y_train)", "execution_count": 16, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Model Evaluation using Test set "}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\n\nyhat = KNNBest.predict(X_test) \nKNNJss = '{:0.2f}'.format(jaccard_similarity_score(y_test, yhat))\nKNNF1 =  '{:0.2f}'.format(f1_score(y_test, yhat, average='weighted'))\n\nyhat = DTreeBest.predict(X_test) \nDTJss =  '{:0.2f}'.format(jaccard_similarity_score(y_test, yhat))\nDTF1 = '{:0.2f}'.format(f1_score(y_test, yhat, average='weighted'))\n\nyhat = SVMBest.predict(X_test) \nSVMJss =  '{:0.2f}'.format(jaccard_similarity_score(y_test, yhat))\nSVMF1 = '{:0.2f}'.format(f1_score(y_test, yhat, average='weighted'))\n\nyhat = LRBest.predict(X_test)\nyhatprob = LRBest.predict_proba(X_test)\nLRJss =  '{:0.2f}'.format(jaccard_similarity_score(y_test, yhat))\nLRF1 = '{:0.2f}'.format(f1_score(y_test, yhat, average='weighted'))\nLRlog = '{:0.2f}'.format(log_loss(y_test, yhatprob))\n\ndata = np.array([['Algorithm','Jaccard', 'F1-score', 'Logloss'],\n                ['KNN',KNNJss,KNNF1,'NA'],\n                ['Decision Tree',DTJss,DTF1,'NA'],\n                ['SVM',SVMJss,SVMF1,'NA'],\n                ['Logistic Regression',LRJss,LRF1,LRlog]\n                \n                ])\n\nReportdf = pd.DataFrame(data=data[1:,1:],\n                  index=data[1:,0],\n                  columns=data[0,1:])\nReportdf.columns.name = 'Algorithm'\n\nprint(Reportdf   )", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "Algorithm           Jaccard F1-score Logloss\nKNN                    0.56     0.54      NA\nDecision Tree          0.56     0.49      NA\nSVM                    0.53     0.50      NA\nLogistic Regression    0.53     0.51    0.68\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Results and Discussion <a name=\"results\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "In the beginning of this notebook, we had categorical data that need to encoded becuase they not a data type that we could have fed through an algoritim, so label encoding was used to created new classes that were of type int8; a numerical data type.\n\nAfter that issue we were presented with another  imbalanced of data. As mentioned earlier, class 1 was nearly three times larger than class 2, So the solution to this was resampling the majority class to 58188 values each.\n\nOnce we analyzed and cleaned the data, it was then fed through three ML models; K-Nearest Neighbor, Decision Tree and Logistic Regression. Although the first two are ideal for this project, logistic regression made most sense because of its binary nature.\n\nEvaluation metrics used to test the accuracy of our models were jaccard index, f-1 score and logloss for logistic regression.\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## Conclusion <a name=\"conclusion\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Based on historical data from weather conditions pointing to certain classes, we can conclude that particular weather conditions have a somewhat impact on whether or not travel could result in property damage (class 1) or injury (class 2)."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}